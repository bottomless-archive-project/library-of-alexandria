= Generator Application

This application's goal is to fill the <<Queue Application>> with downloadable document locations (links).

== Workflow

The Vault Application connects to the <<Queue Application>> application and send URLs to be checked and downloaded there.

These URLs could come from various sources like a local file or from http://commoncrawl.org/[Common Crawl] corpus.

.Parameters
|===
| Parameter | Description

| **loa.database.host**
| The host location of the MongoDB database server. *(Default value: localhost)*

| **loa.database.port**
| The port open for the MongoDB database server. *(Default value: 27017)*

| **loa.database.no-cursor-timeout**
| Whenever the cursor objects created by the application should be able to timeout. Ideally you would set up the timeout on your MongoDB server (see: https://docs.mongodb.com/manual/reference/parameters/#param.cursorTimeoutMillis[cursorTimeoutMillis]) but because not everybody is a MongoDB expert, we disable timeouts by default. This could cause a couple of open cursors (so extra resource usage) on the MongoDB server when the application crashes for some reason, and the cursors are not closed correctly. If you set the cursor timeout too low, then the application will crash if it is not able to process a batch of items under the provided timeout. *(Default value: true)*

| **loa.queue.host**
| The host of the queue where the application should connect, and the document locations should be sent. *(Default value: localhost)*

| **loa.queue.port**
| The port of the queue where the application should connect, and the document locations should be sent. *(Default value: 61616)*

| **loa.source.name**
| The name of the source location. This name will be saved to the database for every crawled document. It could be helpful for statistics collection or in identifying bugs and errors with specific sources. *(Default value: unknown)*

| **loa.source.type**
| Describes the type of the source. Could be either `file` or `commoncrawl`. If the value is file then the location data will be loaded from a local file. If it is commoncrawl then the parser will start loading and parsing urls from the http://commoncrawl.org/[Common Crawl] corpus. *(Default value: file)*

| **loa.source.commoncrawl.crawl-id**
| Used only when `loa.source.type` is set to `commoncrawl`. The id of the Common Crawl crawling sentence. For example the http://commoncrawl.org/2019/01/january-2019-crawl-archive-now-available/[2019 January]'s id is CC-MAIN-2019-04. You can acquire the crawl id for each month's corpus http://commoncrawl.org/the-data/get-started/[here].

| **loa.source.commoncrawl.warc-id**
| Used only when `loa.source.type` is set to `commoncrawl`. Every month's Common Crawl crawling sentence is built from multiple https://en.wikipedia.org/wiki/Web_ARChive[WARC] files (mostly around 64000). This is the id of the WARC file that the parsing should start from. When you first start crawling a month's crawl sequence this should be 1. When the downloader opens a new WARC file it will print its id to the console. If you need to stop the crawler and want to re-start it from where it stopped then write down the last crawled WARC id and set this parameter to it.

| **loa.source.commoncrawl.maximum-record-processors**
| Describes how many threads should be maximally used to parse the WARC records in the WARC files. If you feel that the bottleneck in the document location collecting is the parsing of the WARC files (and not bandwidth) then increase this value. *(Default value: 10)*

| **loa.source.file.location**
| Used only when `loa.source.type` is set to `file`. The location of the source file on the disk. It's not a problem if it contains non-pdf files.

| **loa.source.file.encoding**
| Used only when `loa.source.type` is set to `file`. It can be set to `none` or gzip. If it's set to `none` then the file will be read as a non-compressed file. If it's set to `gzip` then it will be red as a gzipped file, being unzipped on the fly. *(Default value: none)*
|===
